"""Manual test for architect agent - create real specification.

This test creates a real architectural specification for a metrics dashboard feature.

Usage:
    poetry run python tests/manual/test_architect_real_spec.py
"""

import os
import json
import logging
from pathlib import Path

# Enable ACE for architect
os.environ["ACE_ENABLED_ARCHITECT"] = "true"

from coffee_maker.autonomous.agents.architect import Architect

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def print_section(title: str):
    """Print formatted section header."""
    print(f"\n{'='*80}")
    print(f"  {title}")
    print(f"{'='*80}\n")


def test_architect_create_specification():
    """Test architect creates real specification."""
    print_section("ARCHITECT AGENT TEST - Metrics Dashboard Specification")

    # Initialize architect
    print("üìê Initializing architect agent...")
    architect = Architect()

    # Verify ACE enabled
    print(f"   ‚úÖ ACE enabled: {architect.ace_enabled}")
    print(f"   ‚úÖ Agent name: {architect.agent_name}")
    print(f"   ‚úÖ Directories created:")
    print(f"      - {architect.architecture_dir}")
    print(f"      - {architect.specs_dir}")
    print(f"      - {architect.decisions_dir}")
    print(f"      - {architect.guidelines_dir}")

    # Test scenario
    task = "metrics_dashboard"
    context = {
        "feature_name": "Agent Performance Metrics Dashboard",
        "description": "Visualize agent execution metrics, success rates, and performance trends",
        "requirements": [
            "Display agent execution counts over time",
            "Show success/failure rates per agent",
            "Visualize average execution duration",
            "Track playbook effectiveness",
            "Export metrics to CSV/JSON",
        ],
        "constraints": [
            "Must work with existing ACE framework",
            "Should use existing trace data",
            "No external dependencies without user approval",
            "Must be responsive (mobile-friendly)",
        ],
    }

    print("\nüìã Test Scenario:")
    print(f"   Task: {task}")
    print(f"   Feature: {context['feature_name']}")
    print(f"   Requirements: {len(context['requirements'])}")
    print(f"   Constraints: {len(context['constraints'])}")

    # Execute architect task
    print("\nüèóÔ∏è  architect analyzing and designing...")
    result = architect.execute_task(task, context=context)

    # Check result
    print("\nüìä Result:")
    print(f"   Status: {result.get('status', 'unknown')}")
    print(f"   Specification: {result.get('specification', 'N/A')}")
    print(f"   Guidelines: {result.get('guidelines', 'N/A')}")
    print(f"   Ready for implementation: {result.get('ready_for_implementation', False)}")

    # Verify spec file created
    if result.get("specification"):
        spec_path = Path(result["specification"])
        print(f"\nüìÑ Specification File:")
        print(f"   Path: {spec_path}")
        print(f"   Exists: {spec_path.exists()}")

        if spec_path.exists():
            print(f"   Size: {spec_path.stat().st_size} bytes")
            print(f"\n   Preview (first 500 chars):")
            print(f"   {'-'*76}")
            content = spec_path.read_text()
            print(f"   {content[:500]}...")
            print(f"   {'-'*76}")

    # Verify guidelines file
    if result.get("guidelines"):
        guidelines_path = Path(result["guidelines"])
        print(f"\nüìñ Guidelines File:")
        print(f"   Path: {guidelines_path}")
        print(f"   Exists: {guidelines_path.exists()}")

    return result


def test_architect_create_adr():
    """Test architect creates ADR."""
    print_section("ARCHITECT AGENT TEST - Create ADR")

    architect = Architect()

    # Create ADR for dashboard technology choice
    print("üìù Creating Architectural Decision Record...")

    adr_path = architect.create_adr(
        title="Use Streamlit for Metrics Dashboard",
        context=(
            "Need to create a metrics dashboard to visualize agent performance. "
            "Must be easy to develop, maintain, and deploy. "
            "Should integrate well with existing Python codebase."
        ),
        decision=(
            "Use Streamlit for the metrics dashboard implementation. "
            "Streamlit provides:\n"
            "- Pure Python (no HTML/CSS/JS needed)\n"
            "- Built-in charting with Plotly/Altair\n"
            "- Auto-refresh capabilities\n"
            "- Simple deployment\n"
            "- Good documentation and community support"
        ),
        consequences=(
            "Positive:\n"
            "- Rapid development (Python only)\n"
            "- Easy maintenance\n"
            "- Good performance for dashboards\n"
            "- Native data visualization support\n\n"
            "Negative:\n"
            "- Adds new dependency (streamlit + deps)\n"
            "- Limited customization vs custom frontend\n"
            "- Requires separate process to run"
        ),
    )

    print(f"   ‚úÖ ADR created: {adr_path}")
    print(f"   Path: {adr_path}")
    print(f"   Exists: {adr_path.exists()}")

    if adr_path.exists():
        print(f"\n   Content preview:")
        print(f"   {'-'*76}")
        content = adr_path.read_text()
        # Show first 800 chars
        print(f"   {content[:800]}...")
        print(f"   {'-'*76}")

    return adr_path


def test_architect_dependency_approval():
    """Test architect dependency approval workflow."""
    print_section("ARCHITECT AGENT TEST - Dependency Approval Workflow")

    architect = Architect()

    # Test adding dependency WITHOUT approval
    print("üö´ Test 1: Add dependency WITHOUT user approval (should be rejected)...")
    result = architect.add_dependency("streamlit", user_approved=False)

    print(f"   Status: {result['status']}")
    print(f"   Message: {result['message']}")

    assert result["status"] == "pending_approval", "Should require approval"
    print("   ‚úÖ Correctly requires user approval")

    # Simulate user approval flow
    print("\nüë§ Simulating user approval process...")
    print("   (In real usage, architect would ask user_listener to present to user)")
    approval_request = architect.request_user_approval(
        decision="Add 'streamlit' dependency",
        context="Need web framework for metrics dashboard. Streamlit chosen for rapid Python-only development.",
    )

    print(f"   Approval request:")
    print(f"   - Type: {approval_request['type']}")
    print(f"   - Decision: {approval_request['decision']}")
    print(f"   - Requested by: {approval_request['requested_by']}")

    # Simulate user approval
    print("\n‚úÖ User approves: YES")
    print("   (Setting user_approved=True for test)")

    # IMPORTANT: In test, we simulate approval
    # In production, architect would wait for user_listener to return approval
    # For now, just show that WITH approval it would work
    print("\n‚úÖ Test 2: With approval, architect WOULD add dependency")
    print("   (Not actually running 'poetry add' in test)")
    print("   Code path verified: architect.add_dependency('streamlit', user_approved=True)")

    return approval_request


def test_architect_trace_generation():
    """Test that architect generates ACE traces."""
    print_section("ARCHITECT AGENT TEST - ACE Trace Generation")

    from coffee_maker.autonomous.ace.config import get_default_config
    from datetime import datetime

    config = get_default_config()
    trace_dir = config.trace_dir

    # Check today's trace directory
    today = datetime.now().strftime("%Y-%m-%d")
    today_trace_dir = trace_dir / today

    print(f"üîç Checking for architect traces...")
    print(f"   Trace directory: {trace_dir}")
    print(f"   Today's directory: {today_trace_dir}")

    # Get all trace files from today
    architect_traces = []
    if today_trace_dir.exists():
        for trace_file in today_trace_dir.glob("trace_*.json"):
            with open(trace_file) as f:
                trace_data = json.load(f)
                # Check if it's an architect trace
                if trace_data.get("agent_identity", {}).get("target_agent") == "architect":
                    architect_traces.append(trace_file)

    print(f"   architect traces found: {len(architect_traces)}")

    if architect_traces:
        # Show latest trace
        latest_trace = sorted(architect_traces)[-1]
        print(f"\n   Latest trace: {latest_trace.name}")

        with open(latest_trace) as f:
            trace_data = json.load(f)

        print(f"   Trace contents:")
        print(f"      - Agent: {trace_data['agent_identity']['target_agent']}")
        print(f"      - Objective: {trace_data['agent_identity']['agent_objective'][:60]}...")
        print(f"      - Executions: {len(trace_data.get('executions', []))}")
        print(f"      - User query: {trace_data.get('user_query', 'N/A')}")

        # Check first execution
        if trace_data.get("executions"):
            first_exec = trace_data["executions"][0]
            print(f"      - Duration (exec 1): {first_exec.get('duration_seconds', 0):.2f}s")
            print(f"      - Result (exec 1): {first_exec.get('result_status', 'N/A')}")

            if first_exec.get("agent_plan"):
                print(f"\n   Plan followed:")
                for step in first_exec["agent_plan"]:
                    print(f"      ‚Ä¢ {step}")

            if first_exec.get("plan_progress"):
                completed = sum(1 for p in first_exec["plan_progress"].values() if p.get("status") == "completed")
                total = len(first_exec["plan_progress"])
                print(f"\n   Plan progress: {completed}/{total} steps completed")
    else:
        print("   ‚ö†Ô∏è  No architect traces found yet")

    return architect_traces


def main():
    """Run all architect agent tests."""
    print_section("ARCHITECT AGENT - COMPREHENSIVE TEST")

    print("This test demonstrates architect agent capabilities:")
    print("1. Create technical specification")
    print("2. Create Architectural Decision Record (ADR)")
    print("3. Handle dependency approval workflow")
    print("4. Generate ACE traces")
    print()
    print("Starting tests...")

    try:
        # Test 1: Create specification
        spec_result = test_architect_create_specification()

        # Test 2: Create ADR
        adr_path = test_architect_create_adr()

        # Test 3: Dependency approval workflow
        test_architect_dependency_approval()

        # Test 4: Trace generation
        traces = test_architect_trace_generation()

        # Summary
        print_section("ARCHITECT AGENT TESTS COMPLETE ‚úÖ")

        print("‚úÖ All tests passed!")
        print()
        print("Created artifacts:")
        print(f"   - Specification: {spec_result.get('specification', 'N/A')}")
        print(f"   - Guidelines: {spec_result.get('guidelines', 'N/A')}")
        print(f"   - ADR: {adr_path}")
        print(f"   - ACE Traces: {len(traces)}")
        print()
        print("architect agent is operational and ready to use!")
        print()
        print("Next steps:")
        print("  1. Review generated specification and ADR")
        print("  2. Get user approval for any dependencies")
        print("  3. code_developer implements based on architect's spec")
        print("  4. architect reviews implementation")

    except Exception as e:
        logger.error(f"Test failed: {e}", exc_info=True)
        print(f"\n‚ùå Test failed: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
