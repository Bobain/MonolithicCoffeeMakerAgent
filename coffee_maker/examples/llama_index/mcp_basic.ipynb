{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Ppv_z1Tln1",
        "outputId": "d7253003-209c-4d95-cf05-25712988ebc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "time=2025-07-09T14:59:04.032Z level=INFO source=routes.go:1235 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-07-09T14:59:04.032Z level=INFO source=images.go:476 msg=\"total blobs: 0\"\n",
            "time=2025-07-09T14:59:04.032Z level=INFO source=images.go:483 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-07-09T14:59:04.033Z level=INFO source=routes.go:1288 msg=\"Listening on 127.0.0.1:11434 (version 0.9.6)\"\n",
            "time=2025-07-09T14:59:04.033Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-07-09T14:59:04.166Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-7a15f30c-1cfa-798c-e895-b4d3839ebefc library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n"
          ]
        }
      ],
      "source": [
        "! sudo apt-get update -y\n",
        "! sudo apt-get install pciutils\n",
        "! curl -fsSL https://ollama.com/install.sh | sh\n",
        "! ollama serve\n",
        "! ollama pull llama3:8b\n",
        "! pip3 install coffee_maker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy MCP weather tools\n",
        "from coffee_maker.examples.llama_index.dummy_weather_mcp_server import main as run_dummy_weather_server, PORT\n",
        "from coffee_maker.utils.run_deamon_process import run_daemon\n",
        "run_daemon(run_dummy_weather_server, PORT)"
      ],
      "metadata": {
        "id": "DnOldeQLT8yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent with these tools:\n",
        "from coffee_maker.utils.llama_index import get_agent_func_with_context"
      ],
      "metadata": {
        "id": "7-MJV3WrcT29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fhgn-Wk3cT7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4qR_KpNcT_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import asyncio\n",
        "\n",
        "w = widgets.Text(\n",
        "    value='Quel temps fait il à Paris?',\n",
        "    placeholder='Type something',\n",
        "    description='String:',\n",
        "    disabled=False\n",
        ")\n",
        "display(w)\n",
        "\n",
        "button = widgets.Button(description=\"Answer my question\")\n",
        "output = widgets.Output()\n",
        "\n",
        "display(button, output)\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    agent_func = get_agent_func_with_context(f\"http://127.0.0.1:{PORT}\")\n",
        "    with output:\n",
        "        print(agent_func(w.get_state()[\"value\"]))\n",
        "\n",
        "button.on_click(on_button_clicked)"
      ],
      "metadata": {
        "id": "QEnOuXHmcUDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyXyYUXueuOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXEK2nhNss37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOJ416iffCFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o0COENGs3KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import future"
      ],
      "metadata": {
        "id": "_r79EJV2tDaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CCl_NrbgwCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_fn = await get_agent_func_with_context(f\"http://127.0.0.1:{PORT}/sse\")\n",
        "\n",
        "response_generator = agent_fn(message=\"Quel temps fait il à Paris?\", history=[])\n",
        "full_response = \"\"\n",
        "async for token in response_generator:\n",
        "    full_response += token\n",
        "    print(token, end=\"\")"
      ],
      "metadata": {
        "id": "Rz9iMkdMhxy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKJYStr-pbVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxNCBykrpoKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJw7jJpLp3LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYvSOKwhqI-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}